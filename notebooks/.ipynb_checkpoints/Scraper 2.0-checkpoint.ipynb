{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper of Google, Apple and Waze Mobility reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads Google, Apple and Waze Mobility reports, builds cleaned reports in different formats and builds merged files from these sources.\n",
    "\n",
    "Original data:\n",
    "    - Google Community Mobility reports: https://www.google.com/covid19/mobility/\n",
    "    - Apple Mobility Trends reports: https://www.apple.com/covid19/mobility\n",
    "    - Waze COVID-19 local driving trends: https://www.waze.com/covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import zipfile as zp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_link():\n",
    "    '''Get link of Google Community Mobility report file\n",
    "    \n",
    "       Returns:\n",
    "           link (str): link of Google Community report file\n",
    "    '''\n",
    "    # get webpage source\n",
    "    url = 'https://www.google.com/covid19/mobility/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    csv_tag = soup.find('a', {\"class\": \"icon-link\"})\n",
    "    link = csv_tag['href']\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_google_report(directory=\"google_reports\"):\n",
    "    '''Download Google Community Mobility report in CSV format\n",
    "\n",
    "        Args:\n",
    "            directory: directory to which CSV report will be downloaded\n",
    "\n",
    "        Returns:\n",
    "            new_files (bool): flag indicating whether or not new files have been downloaded\n",
    "    '''\n",
    "    new_files = False\n",
    "\n",
    "    # create directory if it don't exist\n",
    "    if not os.path.exists(directory) and directory!='':\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # download CSV file\n",
    "    link = get_google_link()\n",
    "    file_name = \"Global_Mobility_Report.csv\"\n",
    "    path = os.path.join(directory, file_name)\n",
    "    old_size = os.path.getsize(path) if os.path.isfile(path) else 0\n",
    "    urllib.request.urlretrieve(link, path)\n",
    "    new_size = os.path.getsize(path)\n",
    "    if old_size!=new_size:\n",
    "        new_files = True\n",
    "\n",
    "    if not new_files:\n",
    "        print('Google: No updates')\n",
    "    else:\n",
    "        print('Google: Update available')\n",
    "    \n",
    "    return new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_google_report(\n",
    "        source=os.path.join(\"google_reports\", \"Global_Mobility_Report.csv\"),\n",
    "        report_type=\"regions\"):\n",
    "    '''Build cleaned Google report for the worldwide or for some country (currently only for the US)\n",
    "\n",
    "        Args:\n",
    "            source: location of the raw Google CSV report\n",
    "            report_type: two options available: \"regions\" - report for the worldwide, \"US\" - report for the US\n",
    "\n",
    "        Returns:\n",
    "           google (DataFrame): generated Google report\n",
    "    '''\n",
    "    # read the raw report\n",
    "    google = pd.read_csv(source, low_memory=False)\n",
    "    # shorten value column names\n",
    "    google.columns = google.columns.str.replace(\n",
    "        r'_percent_change_from_baseline', '')\n",
    "    # remove underscores from column names\n",
    "    google.columns = google.columns.str.replace(r'_', ' ')\n",
    "    # rename country column\n",
    "    google = google.rename(columns={'country region': 'country'})\n",
    "    if report_type == \"regions\":\n",
    "        # note: temp solution\n",
    "        # remove data of subregions of the second level\n",
    "        google = google[google['sub region 2'].isnull()]\n",
    "        # remove metropolitan data\n",
    "        google = google[google['metro area'].isnull()]\n",
    "        # rename region column\n",
    "        google = google.rename(columns={'sub region 1': 'region'})\n",
    "        google = google.loc[:,\n",
    "                            ['country',\n",
    "                             'region',\n",
    "                             'date',\n",
    "                             'retail and recreation',\n",
    "                             'grocery and pharmacy',\n",
    "                             'parks',\n",
    "                             'transit stations',\n",
    "                             'workplaces',\n",
    "                             'residential']]\n",
    "        google['region'].fillna('Total', inplace=True)\n",
    "    elif report_type == \"US\":\n",
    "        google = google[(google['country'] == \"United States\")]\n",
    "        google = google.rename(\n",
    "            columns={\n",
    "                'sub region 1': 'state',\n",
    "                'sub region 2': 'county'})\n",
    "        google = google.loc[:,\n",
    "                            ['state',\n",
    "                             'county',\n",
    "                             'date',\n",
    "                             'retail and recreation',\n",
    "                             'grocery and pharmacy',\n",
    "                             'parks',\n",
    "                             'transit stations',\n",
    "                             'workplaces',\n",
    "                             'residential']]\n",
    "        google['state'].fillna('Total', inplace=True)\n",
    "        google['county'].fillna('Total', inplace=True)\n",
    "    return google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apple_link():\n",
    "    '''Get link of Apple Mobility Trends report file\n",
    "    \n",
    "       Returns:\n",
    "           link (str): link of Apple Mobility Trends report file\n",
    "    '''\n",
    "    # get link via API\n",
    "    json_link = \"https://covid19-static.cdn-apple.com/covid19-mobility-data/current/v3/index.json\"\n",
    "    with urllib.request.urlopen(json_link) as url:\n",
    "        json_data = json.loads(url.read().decode())\n",
    "    link = \"https://covid19-static.cdn-apple.com\" + \\\n",
    "        json_data['basePath'] + json_data['regions']['en-us']['csvPath']\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_apple_report(directory=\"apple_reports\"):\n",
    "    '''Download Apple Mobility Trends report in CSV\n",
    "\n",
    "        Args:\n",
    "            directory: directory to which CSV report will be downloaded\n",
    "\n",
    "        Returns:\n",
    "            new_files (bool): flag indicating whether or not a new file has been downloaded\n",
    "    '''\n",
    "    new_files = False\n",
    "    \n",
    "    # create directory if it don't exist\n",
    "    if not os.path.exists(directory) and directory!='':\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    link = get_apple_link()\n",
    "    file_name = \"applemobilitytrends.csv\"\n",
    "    path = os.path.join(directory, file_name)\n",
    "    path = os.path.join(directory, file_name)\n",
    "    old_size = os.path.getsize(path) if os.path.isfile(path) else 0\n",
    "    urllib.request.urlretrieve(link, path)\n",
    "    new_size = os.path.getsize(path)\n",
    "    if old_size!=new_size:\n",
    "        new_files = True\n",
    "\n",
    "    if not new_files:\n",
    "        print('Apple: No updates')\n",
    "    else:\n",
    "        print('Apple: Update available')\n",
    "    \n",
    "    return new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_apple_report(\n",
    "    source=os.path.join(\n",
    "        'apple_reports',\n",
    "        \"applemobilitytrends.csv\"),\n",
    "        report_type=\"regions\"):\n",
    "    '''Build cleaned Apple report (transform dates from columns to rows, add country names for subregions and cities)\n",
    "       for the worldwide or for some country (currently only for the US)\n",
    "\n",
    "        Args:\n",
    "            source: location of the raw Apple CSV report\n",
    "            destination: destination file path\n",
    "            report_type: two options available: \"regions\" - report for the worldwide, \"US\" - report for the US\n",
    "\n",
    "        Returns:\n",
    "           apple (DataFrame): generated Apple report\n",
    "    '''\n",
    "    apple = pd.read_csv(source)\n",
    "    apple = apple.drop(columns=['alternative_name'])\n",
    "    apple['country'] = apple.apply(\n",
    "        lambda x: x['region'] if x['geo_type'] == 'country/region' else x['country'],\n",
    "        axis=1)\n",
    "\n",
    "    if report_type == 'regions':\n",
    "        apple = apple[apple.geo_type != 'county']\n",
    "        apple['sub-region'] = apple.apply(lambda x: 'Total' if x['geo_type'] == 'country/region' else (\n",
    "            x['region'] if x['geo_type'] == 'sub-region' else x['sub-region']), axis=1)\n",
    "        apple['subregion_and_city'] = apple.apply(\n",
    "            lambda x: 'Total' if x['geo_type'] == 'country/region' else x['region'], axis=1)\n",
    "        apple = apple.drop(columns=['region'])\n",
    "        apple['sub-region'] = apple['sub-region'].fillna(\n",
    "            apple['subregion_and_city'])\n",
    "\n",
    "        apple = apple.melt(\n",
    "            id_vars=[\n",
    "                'geo_type',\n",
    "                'subregion_and_city',\n",
    "                'sub-region',\n",
    "                'transportation_type',\n",
    "                'country'],\n",
    "            var_name='date')\n",
    "        apple['value'] = apple['value'] - 100\n",
    "\n",
    "        apple = apple.pivot_table(\n",
    "            index=[\n",
    "                \"geo_type\",\n",
    "                \"subregion_and_city\",\n",
    "                \"sub-region\",\n",
    "                \"date\",\n",
    "                \"country\"],\n",
    "            columns='transportation_type').reset_index()\n",
    "        apple.columns = [t + (v if v != \"value\" else \"\")\n",
    "                         for v, t in apple.columns]\n",
    "        apple = apple.loc[:,\n",
    "                          ['country',\n",
    "                           'sub-region',\n",
    "                           'subregion_and_city',\n",
    "                           'geo_type',\n",
    "                           'date',\n",
    "                           'driving',\n",
    "                           'transit',\n",
    "                           'walking']]\n",
    "        apple = apple.sort_values(by=['country',\n",
    "                                      'sub-region',\n",
    "                                      'subregion_and_city',\n",
    "                                      'date']).reset_index(drop=True)\n",
    "    elif report_type == \"US\":\n",
    "        apple = apple[apple.country == \"United States\"].drop(columns=[\n",
    "                                                             'country'])\n",
    "        apple['sub-region'] = apple['sub-region'].fillna(\n",
    "            apple['region']).replace({\"United States\": \"Total\"})\n",
    "        apple['region'] = apple.apply(lambda x: x['region'] if (\n",
    "            x['geo_type'] == 'city' or x['geo_type'] == 'county') else 'Total', axis=1)\n",
    "        apple = apple.rename(\n",
    "            columns={\n",
    "                'sub-region': 'state',\n",
    "                'region': 'county_and_city'})\n",
    "\n",
    "        apple = apple.melt(\n",
    "            id_vars=[\n",
    "                'geo_type',\n",
    "                'state',\n",
    "                'county_and_city',\n",
    "                'transportation_type'],\n",
    "            var_name='date')\n",
    "        apple['value'] = apple['value'] - 100\n",
    "\n",
    "        apple = apple.pivot_table(\n",
    "            index=[\n",
    "                'geo_type',\n",
    "                'state',\n",
    "                'county_and_city',\n",
    "                'date'],\n",
    "            columns='transportation_type').reset_index()\n",
    "        apple.columns = [t + (v if v != \"value\" else \"\")\n",
    "                         for v, t in apple.columns]\n",
    "\n",
    "        apple = apple.loc[:, ['state', 'county_and_city', 'geo_type',\n",
    "                              'date', 'driving', 'transit', 'walking']]\n",
    "        apple = apple.sort_values(\n",
    "            by=['state', 'county_and_city', 'geo_type', 'date']).reset_index(drop=True)\n",
    "    return apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waze_links():\n",
    "    '''Get links of raw Waze local driving Trends report files\n",
    "    \n",
    "       Returns:\n",
    "           link (list): links of Waze local driving Trends report files\n",
    "    '''\n",
    "    github_link = \"https://raw.githubusercontent.com/ActiveConclusion/waze_mobility_scraper/master/\"\n",
    "    country_level_link = github_link + \"Waze_Country-Level_Data.csv\"\n",
    "    city_level_link = github_link + \"Waze_City-Level_Data.csv\"\n",
    "    return [country_level_link, city_level_link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_waze_reports(directory=\"waze_reports\"):\n",
    "    '''Download Waze local driving Trends reports in CSV\n",
    "\n",
    "        Args:\n",
    "            directory: directory to which CSV reports will be downloaded\n",
    "\n",
    "        Returns:\n",
    "            new_files (bool): flag indicating whether or not a new files has been downloaded\n",
    "    '''\n",
    "    new_files = False\n",
    "    \n",
    "    # create directory if it don't exist\n",
    "    if not os.path.exists(directory) and directory!='':\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    links = get_waze_links()\n",
    "    file_names = [\"Waze_Country-Level_Data.csv\", \"Waze_City-Level_Data.csv\"]\n",
    "    file_links = dict(zip(file_names, links))\n",
    "    for file_name in file_names: \n",
    "        path = os.path.join(directory, file_name)\n",
    "        link = file_links[file_name]\n",
    "        old_size = os.path.getsize(path) if os.path.isfile(path) else 0\n",
    "        urllib.request.urlretrieve(link, path)\n",
    "        new_size = os.path.getsize(path)\n",
    "        if old_size!=new_size:\n",
    "            new_files = True\n",
    "\n",
    "    if not new_files:\n",
    "        print('Waze: No updates')\n",
    "    else:\n",
    "        print('Waze: Update available')\n",
    "    \n",
    "    return new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_waze_report(countries_source=os.path.join(\"waze_reports\", \"Waze_Country-Level_Data.csv\"),\n",
    "                     cities_source=os.path.join(\"waze_reports\", \"Waze_City-Level_Data.csv\")):\n",
    "    '''Build cleaned Waze report (transform dates from string to date format, merge country&city-level data,\n",
    "        add geo_type column)\n",
    "\n",
    "        Args:\n",
    "            countries_source: location of the raw Waze country-level CSV report\n",
    "            cities_source: location of the raw Waze city-level CSV report\n",
    "\n",
    "        Returns:\n",
    "           waze (DataFrame): generated Waze report\n",
    "    '''\n",
    "    waze_countries = pd.read_csv(countries_source, parse_dates=['Date'])\n",
    "    waze_cities = pd.read_csv(cities_source, parse_dates=['Date'])\n",
    "    waze_countries['City'] = 'Total'\n",
    "    waze_countries['geo_type'] = 'country'\n",
    "    waze_cities['geo_type'] = 'city'\n",
    "    \n",
    "    waze = waze_countries.append(waze_cities)\n",
    "    waze = waze.rename(columns={'Country':'country', 'City':'city', \n",
    "                                'Date':'date', '% Change In Waze Driven Miles/KMs':'driving_waze'})\n",
    "    waze['driving_waze'] = waze['driving_waze'] * 100\n",
    "    waze['date'] = waze['date'].dt.date\n",
    "    waze = waze.loc[:,['country', 'city','geo_type','date', 'driving_waze']]\n",
    "    waze = waze.sort_values(by=['country', 'city', 'geo_type', 'date']).reset_index(drop=True)\n",
    "    return waze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary_report(apple_source, google_source, report_type=\"regions\"):\n",
    "    '''Build a merged report from Google and Apple data\n",
    "\n",
    "        Args:\n",
    "            apple_source: location of the CSV report generated by build_apple_report function\n",
    "            google_source: location of the CSV report generated by build_google_report function\n",
    "            report_type: two options available: \"regions\" - report for the worldwide, \"US\" - report for the US\n",
    "\n",
    "        Returns:\n",
    "            summary (DataFrame): merged report from Google and Apple data\n",
    "    '''\n",
    "    apple = pd.read_csv(apple_source, low_memory=False)\n",
    "    google = pd.read_csv(google_source, low_memory=False)\n",
    "    summary = pd.DataFrame()\n",
    "    # build report for regions\n",
    "    if report_type == \"regions\":\n",
    "        apple = apple.rename(columns={'subregion_and_city': 'region'})\n",
    "        apple = apple.loc[:, ['country', 'region',\n",
    "                              'date', 'driving', 'transit', 'walking']]\n",
    "        # get matching table for converting Apple countries and subregions to\n",
    "        # Google names\n",
    "        country_AtoG_file = os.path.join(\n",
    "            'auxiliary_data', 'country_Apple_to_Google.csv')\n",
    "        subregions_AtoG_file = os.path.join(\n",
    "            'auxiliary_data', 'subregions_Apple_to_Google.csv')\n",
    "\n",
    "        if os.path.isfile(country_AtoG_file):\n",
    "            country_AtoG = pd.read_csv(country_AtoG_file, index_col=0)\n",
    "        else:\n",
    "            country_AtoG = None\n",
    "        if os.path.isfile(subregions_AtoG_file):\n",
    "            subregions_AtoG = pd.read_csv(subregions_AtoG_file, index_col=0)\n",
    "        else:\n",
    "            subregions_AtoG = None\n",
    "        # convert Apple countries and subregions to Google names\n",
    "        apple['country'] = apple.apply(lambda x: country_AtoG.loc[x['country'], 'country_google'] if (\n",
    "            country_AtoG is not None and x['country'] in country_AtoG.index) else x['country'], axis=1)\n",
    "        apple['region'] = apple.apply(lambda x: subregions_AtoG.loc[x['region'], 'subregion_Google'] if (\n",
    "            subregions_AtoG is not None and x['region'] in subregions_AtoG.index) else x['region'], axis=1)\n",
    "        # merge reports\n",
    "        apple = apple.set_index(['country', 'region', 'date'])\n",
    "        google = google.set_index(['country', 'region', 'date'])\n",
    "        summary = google.join(apple, how='outer')\n",
    "        summary = summary.reset_index(level=['country', 'region', 'date'])\n",
    "    elif report_type == \"US\":\n",
    "        apple = apple.loc[:, ['state', 'county_and_city',\n",
    "                              'date', 'driving', 'transit', 'walking']]\n",
    "        apple.loc[apple.state == 'Washington DC',\n",
    "                  'state'] = 'District of Columbia'\n",
    "        apple.loc[apple.county_and_city ==\n",
    "                  'Washington DC', 'county_and_city'] = 'Total'\n",
    "\n",
    "        google = google.rename(columns={'county': 'county_and_city'})\n",
    "        # merge reports\n",
    "        apple = apple.set_index(['state', 'county_and_city', 'date'])\n",
    "        google = google.set_index(['state', 'county_and_city', 'date'])\n",
    "        summary = google.join(apple, how='outer')\n",
    "        summary = summary.reset_index(\n",
    "            level=['state', 'county_and_city', 'date'])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-49812adaae57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mzf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Global_Mobility_Report.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"google_reports\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# download new report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mnew_files_status_google\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_google_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnew_files_status_google\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# build reports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-dcea48cbadb4>\u001b[0m in \u001b[0;36mdownload_google_report\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mold_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mnew_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mold_size\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1010\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1012\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    872\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# process Google reports\n",
    "GOOGLE_ZIP_PATH = os.path.join(\"google_reports\", \"Global_Mobility_Report.zip\")\n",
    "GOOGLE_CSV_PATH = os.path.join(\"google_reports\", \"Global_Mobility_Report.csv\")\n",
    "# unzip existing report\n",
    "if os.path.exists(GOOGLE_ZIP_PATH):\n",
    "    with zp.ZipFile(GOOGLE_ZIP_PATH, 'r') as zf:\n",
    "        zf.extract('Global_Mobility_Report.csv', \"google_reports\")\n",
    "# download new report\n",
    "new_files_status_google = download_google_report()\n",
    "if new_files_status_google:\n",
    "    # build reports\n",
    "    google_world = build_google_report()\n",
    "    google_US = build_google_report(report_type=\"US\")\n",
    "    # write reports to CSV and Excel\n",
    "    google_world.to_csv(os.path.join(\"google_reports\", \"mobility_report_countries.csv\"), index=False)\n",
    "    google_world.to_excel(os.path.join(\"google_reports\", \"mobility_report_countries.xlsx\"), \n",
    "                          index=False, sheet_name='Data', engine = 'xlsxwriter')\n",
    "    google_US.to_csv(os.path.join(\"google_reports\", \"mobility_report_US.csv\"), index=False)\n",
    "    google_US.to_excel(os.path.join(\"google_reports\", \"mobility_report_US.xlsx\"), \n",
    "                          index=False, sheet_name='Data', engine = 'xlsxwriter')\n",
    "    # zip raw report\n",
    "    with zp.ZipFile(GOOGLE_ZIP_PATH, 'w', zp.ZIP_DEFLATED) as zf:\n",
    "        zf.write(GOOGLE_CSV_PATH,\"Global_Mobility_Report.csv\")\n",
    "# delete raw CSV report\n",
    "os.remove(GOOGLE_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process Apple reports\n",
    "# download new report\n",
    "new_files_status_apple = download_apple_report()\n",
    "if new_files_status_apple:\n",
    "    # build reports\n",
    "    apple_world = build_apple_report()\n",
    "    apple_US = build_apple_report(report_type=\"US\")\n",
    "    # write reports to CSV and Excel\n",
    "    apple_world.to_csv(os.path.join(\"apple_reports\", \"apple_mobility_report.csv\"), index=False)\n",
    "    apple_world.to_excel(os.path.join(\"apple_reports\", \"apple_mobility_report.xlsx\"), \n",
    "                         index=False, sheet_name='Data', engine = 'xlsxwriter')\n",
    "    apple_US.to_csv(os.path.join(\"apple_reports\", \"apple_mobility_report_US.csv\"), index=False)\n",
    "    apple_US.to_excel(os.path.join(\"apple_reports\", \"apple_mobility_report_US.xlsx\"), \n",
    "                      index=False, sheet_name='Data', engine = 'xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process Waze reports\n",
    "# download new report\n",
    "new_files_status_waze = download_waze_reports()\n",
    "if new_files_status_waze:\n",
    "    # build report\n",
    "    waze = build_waze_report()\n",
    "    # write report to CSV and Excel\n",
    "    waze.to_csv(os.path.join(\"waze_reports\", \"waze_mobility.csv\"), index=False)\n",
    "    waze.to_excel(os.path.join(\"waze_reports\", \"waze_mobility.xlsx\"),\n",
    "                  index=False, sheet_name='Data', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build summary reports\n",
    "if new_files_status_apple or new_files_status_google:\n",
    "    print(\"Merging reports...\")\n",
    "    summary_regions = build_summary_report(os.path.join(\"apple_reports\",\"apple_mobility_report.csv\"),\n",
    "                                          os.path.join(\"google_reports\", \"mobility_report_countries.csv\"))\n",
    "    summary_US = build_summary_report(os.path.join(\"apple_reports\", \"apple_mobility_report_US.csv\"), \n",
    "                                      os.path.join(\"google_reports\", \"mobility_report_US.csv\"), 'US')\n",
    "    summary_countries = summary_regions[summary_regions['region']=='Total'].drop(columns=['region'])\n",
    "    \n",
    "    print('Writing merged reports to files...')\n",
    "    summary_regions.to_csv(os.path.join(\"summary_reports\", \"summary_report_regions.csv\"), index=False)\n",
    "    summary_regions.to_excel(os.path.join(\"summary_reports\", \"summary_report_regions.xlsx\"), \n",
    "                             index=False, sheet_name='Data', engine = 'xlsxwriter')\n",
    "    summary_US.to_csv(os.path.join(\"summary_reports\", \"summary_report_US.csv\"), index=False)\n",
    "    summary_US.to_excel(os.path.join(\"summary_reports\", \"summary_report_US.xlsx\"),\n",
    "                        index=False, sheet_name='Data', engine = 'xlsxwriter')\n",
    "    summary_countries.to_csv(os.path.join(\"summary_reports\", \"summary_report_countries.csv\"), index=False)\n",
    "    summary_countries.to_excel(os.path.join(\"summary_reports\", \"summary_report_countries.xlsx\"),\n",
    "                               index=False, sheet_name='Data', engine = 'xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_raw = pd.read_csv(os.path.join(\"google_reports\", \"Global_Mobility_Report.zip\"), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_no_US = google_raw[google_raw.country_region != 'United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_region</th>\n",
       "      <th>sub_region_1</th>\n",
       "      <th>sub_region_2</th>\n",
       "      <th>date</th>\n",
       "      <th>retail_and_recreation_percent_change_from_baseline</th>\n",
       "      <th>grocery_and_pharmacy_percent_change_from_baseline</th>\n",
       "      <th>parks_percent_change_from_baseline</th>\n",
       "      <th>transit_stations_percent_change_from_baseline</th>\n",
       "      <th>workplaces_percent_change_from_baseline</th>\n",
       "      <th>residential_percent_change_from_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         country_region sub_region_1 sub_region_2        date  \\\n",
       "0  United Arab Emirates          NaN          NaN  2020-02-15   \n",
       "1  United Arab Emirates          NaN          NaN  2020-02-16   \n",
       "2  United Arab Emirates          NaN          NaN  2020-02-17   \n",
       "3  United Arab Emirates          NaN          NaN  2020-02-18   \n",
       "4  United Arab Emirates          NaN          NaN  2020-02-19   \n",
       "\n",
       "   retail_and_recreation_percent_change_from_baseline  \\\n",
       "0                                                0.0    \n",
       "1                                                1.0    \n",
       "2                                               -1.0    \n",
       "3                                               -2.0    \n",
       "4                                               -2.0    \n",
       "\n",
       "   grocery_and_pharmacy_percent_change_from_baseline  \\\n",
       "0                                                4.0   \n",
       "1                                                4.0   \n",
       "2                                                1.0   \n",
       "3                                                1.0   \n",
       "4                                                0.0   \n",
       "\n",
       "   parks_percent_change_from_baseline  \\\n",
       "0                                 5.0   \n",
       "1                                 4.0   \n",
       "2                                 5.0   \n",
       "3                                 5.0   \n",
       "4                                 4.0   \n",
       "\n",
       "   transit_stations_percent_change_from_baseline  \\\n",
       "0                                            0.0   \n",
       "1                                            1.0   \n",
       "2                                            1.0   \n",
       "3                                            0.0   \n",
       "4                                           -1.0   \n",
       "\n",
       "   workplaces_percent_change_from_baseline  \\\n",
       "0                                      2.0   \n",
       "1                                      2.0   \n",
       "2                                      2.0   \n",
       "3                                      2.0   \n",
       "4                                      2.0   \n",
       "\n",
       "   residential_percent_change_from_baseline  \n",
       "0                                       1.0  \n",
       "1                                       1.0  \n",
       "2                                       1.0  \n",
       "3                                       1.0  \n",
       "4                                       1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_no_US['sub_region_1'] = google_no_US.apply(lambda x: x['sub_region_1'] if isinstance(x['sub_region_1'],str)\n",
    "                                                  else x['metro_area'], axis=1)\n",
    "\n",
    "google_no_US = google_no_US.drop(columns=['country_region_code', 'metro_area', 'iso_3166_2_code', 'census_fips_code'])\n",
    "google_no_US.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This sheet is too large! Your sheet size is: 1109483, 10 Max sheet size is: 1048576, 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1e4835f8c2ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m google_no_US.to_excel(os.path.join(\"google_reports\", \"mobility_report_no_US.xlsx\"), \n\u001b[1;32m----> 2\u001b[1;33m                           index=False, sheet_name='Data', engine = 'xlsxwriter')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[0;32m   2179\u001b[0m             \u001b[0mstartcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2180\u001b[0m             \u001b[0mfreeze_panes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2181\u001b[1;33m             \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2182\u001b[0m         )\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[0;32m    717\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_rows\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_rows\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_cols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             raise ValueError(\n\u001b[1;32m--> 719\u001b[1;33m                 \u001b[1;34mf\"This sheet is too large! Your sheet size is: {num_rows}, {num_cols} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m                 \u001b[1;34mf\"Max sheet size is: {self.max_rows}, {self.max_cols}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: This sheet is too large! Your sheet size is: 1109483, 10 Max sheet size is: 1048576, 16384"
     ]
    }
   ],
   "source": [
    "google_no_US.to_excel(os.path.join(\"google_reports\", \"mobility_report_no_US.xlsx\"), \n",
    "                          index=False, sheet_name='Data', engine = 'xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_no_US.to_csv(os.path.join(\"google_reports\", \"mobility_report_no_US.csv\"), \n",
    "                          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
